{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotdog or Not Hotdog?\n",
    "\n",
    "---\n",
    "\n",
    " Context:\n",
    " https://www.youtube.com/watch?v=pqTntG1RXSY\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "The task was to predict whether an image was either a hotdog, or not a hotdog, based on a Linear Regression algorithm. We look at the difference between doing this from a fixed-location pixel values, and from the embedded result from a neural network.\n",
    "\n",
    "Our dataset was provided by DanB on Kaggle: https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog/download \n",
    "\n",
    "\n",
    "## Part 1: Pixel Positional Linear Regression\n",
    "\n",
    "We start by setting this up with a basic 1 dimensional array based on rescaled 16 x 16 grayscale images.\n",
    "\n",
    "## Part 2: Neural Network Embedding Linear Regression\n",
    "\n",
    "Next, try the same regression, but start by inputting 150 x 150 colour images into ImageNet. We run the linear regression on the embedded output from the neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center><h5> Part 1: Pixel Positional Linear Regression </h5></center>\n",
    "---\n",
    "###Â Imports\n",
    "Start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Loading Data\n",
    "\n",
    "Start by heading to the source dataset on Kaggle, and downloading the zip file. \n",
    "\n",
    "Then unzip the file and relocate it to the same directory as your code.\n",
    "\n",
    "Once that's done, you can load the dataset with sklearn. (Because the content is not text, we set the load_content to False, so sklearn doesn't try to automatically load everything).\n",
    "\n",
    "*NB: The notebook seperates a training file and a test file by default. I've moved all the .jpgs into the relevent folder within train, and discarded the test folder)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the Folder with sklearn. \n",
    "folder = sklearn.datasets.load_files(\"./hot-dog-not-hot-dog/seefood/train\",\n",
    "                                     load_content=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Setting up the data\n",
    "\n",
    "Next, we setup our data into a useable numpy array format.\n",
    "\n",
    "**IMAGE_SIZE** is the dimensions of the image we are resizing to (we're starting with 16)\n",
    "\n",
    "Our **xs** will be a 2d numpy array, which contains: \n",
    "\n",
    "       - 498 arrays (the total number of images)\n",
    "       - 256 integers / array (the pixels for each image)\n",
    "       \n",
    "Our **ys** will by a 1d numpy array, which contains:\n",
    "\n",
    "       - 498 integers (0 or 1) representing whether the target for each image is *hotdog* or *not hotdog*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "\n",
    "# Setup arrays\n",
    "xs = np.zeros((len(folder.filenames), IMAGE_SIZE ** 2))\n",
    "ys = np.array(folder.target)\n",
    "\n",
    "# print(xs.shape)\n",
    "# print(ys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Formating the images and adding to arrays\n",
    "\n",
    "Now we iterate over all the files in the loaded folder. \n",
    "\n",
    "For each, we're using sklearn to read the image as black & white, then using OpenCV (cv2) to resize the image to 16 x 16.\n",
    "\n",
    "The **lowresim** will return a 2d (16x16) array. So we'll need to flatten this using the reshape method.\n",
    "\n",
    "Finally, we set the index of **xs** to our flat array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate over the loaded folder and format images\n",
    "for i, file in enumerate(folder.filenames):   \n",
    "    im = imread(file, as_gray=True)\n",
    "    lowresim = cv2.resize(im, dsize=(IMAGE_SIZE,IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\n",
    "    reshaped = lowresim.reshape(IMAGE_SIZE ** 2)\n",
    "    xs[i] = reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Splitting the training data and testing data\n",
    "\n",
    "It's important that we seperate our training data from the testing data, so that leave some unseen images to test with. We'll do this with sklearn's train_test_split. \n",
    "\n",
    "We set our x_train (The images) and y_train (The classification targets) as seperate from the x_test and y_test. In this case we're saving 20% for our testing data. \n",
    "\n",
    "*NB: Using random state allows us to keep state, so that same randomization function is used. This way for any input, our random output will be constant.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20, random_state=0)\n",
    "\n",
    "# print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training the model\n",
    "\n",
    "Time for the magic to happen.\n",
    "\n",
    "More on sklearn's LogisticRegression here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Verify the score of our Model\n",
    "\n",
    "Finally we set a **score** to verify our use the model to test images against the test targets.\n",
    "\n",
    "Given the complexity and noise of the images, we get a pretty dismal score of 0.56 (only 6% better than random guessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the score\n",
    "score = logisticRegr.score(x_test, y_test)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center><h5> Part 2: Neural Network Embedding Linear Regression </h5></center>\n",
    "---\n",
    "Based on the output above, the main difficulties relate to the insufficiency of pixel positioning. These would include: \n",
    "    \n",
    "        - The background noise in the Photo\n",
    "        - The positioning and angle of the hotdogs\n",
    "\n",
    "So, rather than relying on the grayscale value of each pixel, instead, we can feed our images into a pre-trained neural network, and take an output value from the network. This should provide us a more position agnostic value, with better information on the content of the photo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Import the Neural Network libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Flatten, Dense, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.applications import inception_v3\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Setup the pre-trained network\n",
    "\n",
    "Imagenet is a pretrained neural network from Keras, which has 1000 classifications from a large dataset of images. \n",
    "\n",
    "http://www.image-net.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inception model\n",
    "inception = inception_v3.InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(150, 150, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Setup our Model\n",
    "\n",
    "Now, we setup a model which will input our images into the imagenet network, and output a flattened array based on the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our Model\n",
    "x = Flatten()(inception.output)\n",
    "model = Model(input=inception.input, output=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Setup the data for the network\n",
    "\n",
    "We'll take the same code as above, but refactor it to provide a larger 150 x 150 colour image.\n",
    "\n",
    "In this case, each pixel value will be a 3 integer array (RGB), we'll also need to reshape the pixel values so that each is a number between 0 - 1, rather than 0 - 255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 150\n",
    "\n",
    "# Setup array for images\n",
    "xs = np.zeros((len(folder.filenames), IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "# Setup array for targets\n",
    "ys = np.array(folder.target)\n",
    "\n",
    "# Iterate over the loaded folder and format images\n",
    "for i, file in enumerate(folder.filenames):   \n",
    "    im = imread(file)\n",
    "    lowresim = cv2.resize(im, dsize=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                          interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    reshaped = np.divide(lowresim, 255.0) \n",
    "    xs[i] = reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Retrive the Outputs from the Neural Net\n",
    "\n",
    "We can now feed all of the images in **xs** into our model, and store this in an array called **embeddings**.\n",
    "\n",
    "Embeddings will be a 2d array, which contains 498 flattened network outputs, each, 18,432 numbers long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the images through the neural network\n",
    "embeddings = model.predict(xs)\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Split the training data and testing data\n",
    "\n",
    "Same as in Part 1, but this time using the embedding results rather than the pixel values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(embeddings, ys, test_size=0.20, random_state=0)\n",
    "\n",
    "# print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Train the Model and Verify Score\n",
    "\n",
    "We'll use the same Linear Regression algorithm as in Part 1, and check the score the same.\n",
    "\n",
    "Expected Output: 0.89\n",
    "\n",
    "*33% more accurate than the method used in Part 1!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Check the score\n",
    "score = logisticRegr.score(x_test, y_test)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center><h5> Outcomes </h5></center>\n",
    "---\n",
    "\n",
    "Because the embedding from the Neural Network provides a more contextually aware output for the contents of the image (for example, accounting for things like positioning) our result is dramatically improved by catagorizing the objects based on the embedding, rather than on a location dependent pixel value. \n",
    "\n",
    "One unfortunate factor to note, the results may be skewed by the nature of ImageNet, as this network includes 'hotdogs' as one of it's 1000 catagories. It also provides no way to exclude this from the weights used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Michael Stresing** <br>\n",
    "*November 2019*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
