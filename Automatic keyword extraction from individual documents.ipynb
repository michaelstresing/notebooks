{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic keyword extraction from individual documents\n",
    "\n",
    "https://www.researchgate.net/profile/Stuart_Rose/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents/links/59edf51fa6fdccbbefd5434a/Automatic-Keyword-Extraction-from-Individual-Documents.pdf?origin=publication_detail\n",
    "\n",
    "Keywords = one + words which compactly represent a document. Independant of a *corpus*, and can be applied more widely than *mathematical signatures*.\n",
    "\n",
    "Key application is in IR's (Information Retrival) systems. \n",
    "\n",
    "4\n",
    "Mostly not used, most approaches are manual.\n",
    "\n",
    "Earliest approaches = select a vocabulary based on statistically discriminating words across a corpus.\n",
    "\n",
    "This is bad, as only operates on single words and single words can have multiple differing contexts.\n",
    "\n",
    "So, instead, focus on document-oriented methods. (Agnostic of the corpus). Prior D-O methods identify part-of-speech + ML.\n",
    "\n",
    "(Examples presented -> Nout-phrase chunks, syntactic filters, chi-sqaure measure to selective words.)\n",
    "\n",
    "5\n",
    "This article is about RAKE (Rapid Automatic Keyword Extraction). Unsupervised, domain-independent, language-independent. \n",
    "\n",
    "Goal - effecient D-O method, extendable, applicable to multiple types of docs. \n",
    "\n",
    "6\n",
    "Stop words (stoplist) is the inspiration for RAKE. RAKE uses stopwords to delimit the document into candidate keywords. \n",
    "\n",
    "Co-occurrences of candidate keywords is meaningful.\n",
    "\n",
    "Starts by parsing text into a set of keywords.\n",
    "Document -> array of words -> sequences of contiguous works.\n",
    "\n",
    "7\n",
    "Keywords are graphed, and co-occurrences shown. Score for keywords is sum of it's member word scores. \n",
    "\n",
    "8\n",
    "Issue with split keywords with stopwords (axis *of* evil).\n",
    "To solve: RAKE looks for keywords which adjoin at least twice in document in same order. Then creates new keyword of it. \n",
    "\n",
    "Once scored, top T keywords are used. (T is 1/3 the number of words in graph). \n",
    "\n",
    "9\n",
    "Percision, Recall, F-measure -> 67%, 86%, 75%\n",
    "Benchmarked, RAKE has better percision and F, lower Recall. \n",
    "\n",
    "10\n",
    "(Perfect precision not possible by any technique, as some keywords don't appear in documents at all)\n",
    "\n",
    "\n",
    "11\n",
    "Much more time effecient, especially as content of document increases, as scores are done in a single pass vs. multiple iterations to get convergence. (160ms vs 1002ms for 500 abstracts)\n",
    "\n",
    "12\n",
    "Stoplist generation, lacks rigour in creation. Often based on general set then tuned to applications or domains. \n",
    "\n",
    "Selecting by term frequency is an issue - ex. system, control, method are very frequent in some abstracts. \n",
    "\n",
    "Better method: words adjecent to (not within) keywords, are less likely to be meaningful, so thus, better choices for stop words. \n",
    "\n",
    "-> identify for each abstract the words adjacent to words in the uncontrolled key-word list. Frequency of each accumulated. \n",
    "\n",
    "Words occurring more frequently in keywords than adjacent were excluded from stoplist.\n",
    "\n",
    "14\n",
    "Testing -> make 3 stoplists via term frequency 3 via method above (subtract where adjecent < keywords).\n",
    "...No surprise, latter is better on all evaluation metrics.\n",
    "\n",
    "Goal would be to generate stoplist once per domain, and then use with RAKE on any future articles. \n",
    "\n",
    "15\n",
    "Tested on newsarticles. \n",
    "Subjects are often referred to in longer form infrequently, then in short form frequently.\n",
    "\n",
    "17\n",
    "Look at exclusivity (which is extracted frequency / referenced frequency)\n",
    "\n",
    "About half in set were extracted from every document referenced. (or exclusivity = 1)\n",
    "\n",
    "Of those, essentiality is exclusivity * number extracted. (How essential to corpus of documents).\n",
    "\n",
    "18\n",
    "Next, generality is frequency referenced * (1 - exclusivity). (How often referenced but not extracted). \n",
    "\n",
    "19\n",
    "Summary = RAKE is more precise and approx similar recall to other NLP methods. As it uses a single pass it's much more effecient. Also, the simplicity make it suitable for a wide range of applications.\n",
    "\n",
    "Basically, RAKE is pretty sweet...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
